## 安装

设置环境变量，一般必须设置，因为要自定义airflow 数据存储位置。环境变量必须在服务启动前设置

```Bash
# 指定 airflow 数据文件存储位置
export AIRFLOW_HOME=~/airflow
# 为 dag 文件 import 自定义模块做准备
export PYTHONPATH=$PYTHONPATH:~/airflow/dags
```

## install

```Bash
pip install apache-airflow
```

### 创建数据库

```sql
CREATE DATABASE airflow CHARACTER SET utf8 COLLATE utf8_unicode_ci;

CREATE USER 'airflow' IDENTIFIED BY 'airflow';

GRANT ALL PRIVILEGES ON airflow.* TO 'airflow';
```

```Bash
airflow db init
```

init 是为了生成默认的 airflow.cfg 文件，但是因为该命令会默认 Init 一个 sqllite db 使用。生产环境可以在 init 只保留配置文件，其余都删掉。

### 配置更改

```Bash
[core]
# dags 文件位置
dags_folder = /data/airflow/dags
# 配置执行器，因为没有太多脚本所以没有用分布式 Celery Executor
executor = LocalExecutor
# 配置 sql 链接
sql_alchemy_conn = mysql+pymysql://user:password@host:port/db
# 不加载示例文件
load_examples = False
# False:执行任务前重新加载 plugins
lazy_load_plugins = False
[webserver]
# 邮件中访问连接，设置为访问地址
base_url = http://54.189.249.136:8080

[email]
# 配置邮件内容模板
html_content_template = /data/airflow/email_template/content_template.j2
subject_template = /data/airflow/email_template/subject_template.j2
[smtp]
# 这些根据情况配置
smtp_host = smtp.exmail.qq.com
smtp_starttls = False
smtp_ssl = True
# 切记与 smtp_mail_from 一样，都配置为发件人邮箱
smtp_user = data_public@gmail.com
# 邮箱第三方授权码
smtp_password = xxxx
smtp_port = 465
smtp_mail_from = data_public@gamil.com
smtp_timeout = 30
smtp_retry_limit = 5
```

> 注意：此处使用 SQLAlchemy。所以要安装 pip install SQLAlchemy。Make sure to have specified explicit_defaults_for_timestamp=1

配置文件更新后再执行 `airflow db init` , airflow 会将数据 Init 到配置好的 mysql 中。

### template

<https://gist.github.com/xpeiqi/4dd2153021cfe897ba60b708f6009071>

### dags

创建 dags_folder，根据配置文件中配置的路径创建该目录。

创建用户

```Bash
airflow users create \
    --username admin \
    --firstname camel \
    --lastname data \
    --role Admin \
    -p admin\
    --email spiderman@superhero.org
```

启动服务

```Bash
# 可以不指定端口，默认为 8080
nohup airflow webserver --port 8080 >> /data/airflow/web.log 2>&1 & 
# scheduler 
nohup airflow scheduler >>/data/airflow/scheduler.log 2>&1 & 

```

日志收集

```Bash
# airflow 本身没有 log rotation，所以使用 linux 的 logrotate
# 创建配置文件
cat << EOF >>./scheduler_log_config
/data/airflow/scheduler.log {
    daily
    rotate 7
    missingok
    dateext
    copytruncate
}
EOF 

# test
# -f 选项来强制 logrotate 轮循日志文件，-v 参数提供了详细的输出。
logrotate -vf airflow_webserver
# 把这个命令配置成 job 。cron 或者 airflow dag
logrotate airflow_webserver
```

关闭进程

```Bash
ps -ef|grep 'airflow'|grep -v grep|awk '{print $2}'|xargs kill -9
```

## 常用命令

### list

显示目前存在的 dags ，执行后可以立即添加新增加的 dags。

### delete

删除与指定DAG相关的所有数据库记录。并不删除 dag。所以要删除dag ，要在页面上点删除，还要在 dags 目录中删除文件

### task

```Bash
# airflow tasks test [dag_id] [task_id] [date_id]
airflow tasks run example_bash_operator runme_0 2015-01-01
```

执行 task 测试。

### dags

```Bash
airflow dags backfill example_bash_operator \
    --start-date 2015-01-01 \
    --end-date 2015-01-02
```

执行任务补录。

### db

#### reset

重置数据库信息，但是不会删除用户信息。只是删除 log，dags 相关信息。可以清理数据库用。
